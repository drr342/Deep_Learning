python /home/drr342/dl/assignments/hw3/examples/word_language_model/main.py --data /home/drr342/dl/assignments/hw3/examples/word_language_model/data/wikitext-2 --save /home/drr342/dl/assignments/hw3/models/GRU'_'100'_'1'_'64.pt --cuda --model GRU --epochs 10 --emsize 100 --nlayers 1 --bptt 64

/home/drr342/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
| epoch   1 |   200/ 1631 batches | lr 20.00 | ms/batch 64.30 | loss  8.20 | ppl  3654.41
| epoch   1 |   400/ 1631 batches | lr 20.00 | ms/batch 63.54 | loss  6.78 | ppl   878.68
| epoch   1 |   600/ 1631 batches | lr 20.00 | ms/batch 63.55 | loss  6.37 | ppl   584.79
| epoch   1 |   800/ 1631 batches | lr 20.00 | ms/batch 63.55 | loss  6.16 | ppl   473.53
| epoch   1 |  1000/ 1631 batches | lr 20.00 | ms/batch 63.47 | loss  6.03 | ppl   417.51
| epoch   1 |  1200/ 1631 batches | lr 20.00 | ms/batch 63.61 | loss  5.89 | ppl   360.05
| epoch   1 |  1400/ 1631 batches | lr 20.00 | ms/batch 63.62 | loss  5.82 | ppl   336.50
| epoch   1 |  1600/ 1631 batches | lr 20.00 | ms/batch 63.71 | loss  5.70 | ppl   298.35
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 107.95s | valid loss  5.57 | valid ppl   263.29
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 1631 batches | lr 20.00 | ms/batch 63.93 | loss  5.71 | ppl   301.86
| epoch   2 |   400/ 1631 batches | lr 20.00 | ms/batch 63.78 | loss  5.52 | ppl   249.81
| epoch   2 |   600/ 1631 batches | lr 20.00 | ms/batch 63.93 | loss  5.49 | ppl   242.35
| epoch   2 |   800/ 1631 batches | lr 20.00 | ms/batch 63.81 | loss  5.47 | ppl   236.92
| epoch   2 |  1000/ 1631 batches | lr 20.00 | ms/batch 63.96 | loss  5.44 | ppl   229.39
| epoch   2 |  1200/ 1631 batches | lr 20.00 | ms/batch 63.96 | loss  5.34 | ppl   208.26
| epoch   2 |  1400/ 1631 batches | lr 20.00 | ms/batch 63.99 | loss  5.31 | ppl   202.83
| epoch   2 |  1600/ 1631 batches | lr 20.00 | ms/batch 64.08 | loss  5.25 | ppl   190.91
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 108.44s | valid loss  5.35 | valid ppl   209.63
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 1631 batches | lr 20.00 | ms/batch 64.27 | loss  5.32 | ppl   204.99
| epoch   3 |   400/ 1631 batches | lr 20.00 | ms/batch 64.06 | loss  5.16 | ppl   174.50
| epoch   3 |   600/ 1631 batches | lr 20.00 | ms/batch 64.06 | loss  5.17 | ppl   175.90
| epoch   3 |   800/ 1631 batches | lr 20.00 | ms/batch 64.00 | loss  5.19 | ppl   178.88
| epoch   3 |  1000/ 1631 batches | lr 20.00 | ms/batch 64.15 | loss  5.18 | ppl   177.15
| epoch   3 |  1200/ 1631 batches | lr 20.00 | ms/batch 64.18 | loss  5.09 | ppl   162.99
| epoch   3 |  1400/ 1631 batches | lr 20.00 | ms/batch 64.16 | loss  5.08 | ppl   160.31
| epoch   3 |  1600/ 1631 batches | lr 20.00 | ms/batch 64.24 | loss  5.03 | ppl   153.61
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 108.79s | valid loss  5.23 | valid ppl   186.23
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 1631 batches | lr 20.00 | ms/batch 64.59 | loss  5.11 | ppl   166.03
| epoch   4 |   400/ 1631 batches | lr 20.00 | ms/batch 64.25 | loss  4.96 | ppl   142.66
| epoch   4 |   600/ 1631 batches | lr 20.00 | ms/batch 64.18 | loss  4.98 | ppl   145.57
| epoch   4 |   800/ 1631 batches | lr 20.00 | ms/batch 64.22 | loss  5.02 | ppl   150.90
| epoch   4 |  1000/ 1631 batches | lr 20.00 | ms/batch 64.16 | loss  5.01 | ppl   149.81
| epoch   4 |  1200/ 1631 batches | lr 20.00 | ms/batch 64.12 | loss  4.93 | ppl   138.34
| epoch   4 |  1400/ 1631 batches | lr 20.00 | ms/batch 64.13 | loss  4.91 | ppl   136.31
| epoch   4 |  1600/ 1631 batches | lr 20.00 | ms/batch 64.19 | loss  4.88 | ppl   132.27
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 108.93s | valid loss  5.16 | valid ppl   173.33
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 1631 batches | lr 20.00 | ms/batch 64.31 | loss  4.96 | ppl   143.16
| epoch   5 |   400/ 1631 batches | lr 20.00 | ms/batch 63.98 | loss  4.82 | ppl   123.69
| epoch   5 |   600/ 1631 batches | lr 20.00 | ms/batch 64.02 | loss  4.85 | ppl   127.94
| epoch   5 |   800/ 1631 batches | lr 20.00 | ms/batch 64.01 | loss  4.88 | ppl   131.87
| epoch   5 |  1000/ 1631 batches | lr 20.00 | ms/batch 64.07 | loss  4.89 | ppl   133.47
| epoch   5 |  1200/ 1631 batches | lr 20.00 | ms/batch 63.97 | loss  4.82 | ppl   123.41
| epoch   5 |  1400/ 1631 batches | lr 20.00 | ms/batch 64.09 | loss  4.80 | ppl   121.37
| epoch   5 |  1600/ 1631 batches | lr 20.00 | ms/batch 64.12 | loss  4.77 | ppl   118.35
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 108.65s | valid loss  5.12 | valid ppl   167.53
-----------------------------------------------------------------------------------------
| epoch   6 |   200/ 1631 batches | lr 20.00 | ms/batch 64.29 | loss  4.85 | ppl   127.33
| epoch   6 |   400/ 1631 batches | lr 20.00 | ms/batch 64.11 | loss  4.71 | ppl   110.91
| epoch   6 |   600/ 1631 batches | lr 20.00 | ms/batch 64.10 | loss  4.75 | ppl   115.37
| epoch   6 |   800/ 1631 batches | lr 20.00 | ms/batch 64.02 | loss  4.79 | ppl   120.00
| epoch   6 |  1000/ 1631 batches | lr 20.00 | ms/batch 64.09 | loss  4.80 | ppl   121.20
| epoch   6 |  1200/ 1631 batches | lr 20.00 | ms/batch 63.98 | loss  4.71 | ppl   111.48
| epoch   6 |  1400/ 1631 batches | lr 20.00 | ms/batch 63.99 | loss  4.71 | ppl   110.59
| epoch   6 |  1600/ 1631 batches | lr 20.00 | ms/batch 64.02 | loss  4.69 | ppl   108.35
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 108.67s | valid loss  5.10 | valid ppl   163.40
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 1631 batches | lr 20.00 | ms/batch 64.25 | loss  4.76 | ppl   116.40
| epoch   7 |   400/ 1631 batches | lr 20.00 | ms/batch 64.03 | loss  4.62 | ppl   101.34
| epoch   7 |   600/ 1631 batches | lr 20.00 | ms/batch 64.13 | loss  4.66 | ppl   105.80
| epoch   7 |   800/ 1631 batches | lr 20.00 | ms/batch 64.07 | loss  4.70 | ppl   110.26
| epoch   7 |  1000/ 1631 batches | lr 20.00 | ms/batch 64.11 | loss  4.72 | ppl   112.33
| epoch   7 |  1200/ 1631 batches | lr 20.00 | ms/batch 64.04 | loss  4.64 | ppl   103.48
| epoch   7 |  1400/ 1631 batches | lr 20.00 | ms/batch 63.96 | loss  4.63 | ppl   102.18
| epoch   7 |  1600/ 1631 batches | lr 20.00 | ms/batch 63.76 | loss  4.61 | ppl   100.72
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 108.57s | valid loss  5.10 | valid ppl   164.28
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 1631 batches | lr 5.00 | ms/batch 64.07 | loss  4.62 | ppl   101.48
| epoch   8 |   400/ 1631 batches | lr 5.00 | ms/batch 63.76 | loss  4.44 | ppl    84.62
| epoch   8 |   600/ 1631 batches | lr 5.00 | ms/batch 63.77 | loss  4.45 | ppl    85.39
| epoch   8 |   800/ 1631 batches | lr 5.00 | ms/batch 63.73 | loss  4.45 | ppl    85.78
| epoch   8 |  1000/ 1631 batches | lr 5.00 | ms/batch 63.74 | loss  4.44 | ppl    84.57
| epoch   8 |  1200/ 1631 batches | lr 5.00 | ms/batch 63.75 | loss  4.33 | ppl    75.77
| epoch   8 |  1400/ 1631 batches | lr 5.00 | ms/batch 63.73 | loss  4.28 | ppl    72.16
| epoch   8 |  1600/ 1631 batches | lr 5.00 | ms/batch 63.77 | loss  4.23 | ppl    68.92
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 108.17s | valid loss  4.92 | valid ppl   137.45
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 1631 batches | lr 5.00 | ms/batch 63.99 | loss  4.46 | ppl    86.78
| epoch   9 |   400/ 1631 batches | lr 5.00 | ms/batch 63.75 | loss  4.31 | ppl    74.63
| epoch   9 |   600/ 1631 batches | lr 5.00 | ms/batch 63.78 | loss  4.35 | ppl    77.14
| epoch   9 |   800/ 1631 batches | lr 5.00 | ms/batch 63.80 | loss  4.37 | ppl    79.00
| epoch   9 |  1000/ 1631 batches | lr 5.00 | ms/batch 63.83 | loss  4.37 | ppl    78.96
| epoch   9 |  1200/ 1631 batches | lr 5.00 | ms/batch 63.77 | loss  4.27 | ppl    71.74
| epoch   9 |  1400/ 1631 batches | lr 5.00 | ms/batch 63.74 | loss  4.24 | ppl    69.17
| epoch   9 |  1600/ 1631 batches | lr 5.00 | ms/batch 63.77 | loss  4.21 | ppl    67.25
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 108.19s | valid loss  4.91 | valid ppl   136.19
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 1631 batches | lr 5.00 | ms/batch 63.96 | loss  4.40 | ppl    81.47
| epoch  10 |   400/ 1631 batches | lr 5.00 | ms/batch 63.72 | loss  4.26 | ppl    70.55
| epoch  10 |   600/ 1631 batches | lr 5.00 | ms/batch 63.73 | loss  4.29 | ppl    73.18
| epoch  10 |   800/ 1631 batches | lr 5.00 | ms/batch 63.68 | loss  4.32 | ppl    75.03
| epoch  10 |  1000/ 1631 batches | lr 5.00 | ms/batch 63.70 | loss  4.33 | ppl    75.91
| epoch  10 |  1200/ 1631 batches | lr 5.00 | ms/batch 63.69 | loss  4.23 | ppl    68.91
| epoch  10 |  1400/ 1631 batches | lr 5.00 | ms/batch 63.72 | loss  4.20 | ppl    66.90
| epoch  10 |  1600/ 1631 batches | lr 5.00 | ms/batch 63.72 | loss  4.18 | ppl    65.61
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 108.07s | valid loss  4.90 | valid ppl   134.84
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  4.84 | test ppl   126.47
=========================================================================================
