\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{main}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{main}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{enumerate}
\usepackage[linguistics]{forest}
\usepackage{adjustbox}
\usepackage{amssymb}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfoor}{\rfloor}
%\usepackage[margin=0.5in]{geometry}
%\DeclareMathOperator*{\argmax}{argmax}

\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\ttfamily\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
}
 
\lstset{style=mystyle}

\title{Deep Learning - Spring 2019\\
       \Large Homework 1}
\graphicspath{{images/}}
\setcitestyle{round, sort, numbers}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Daniel Rivera Ruiz\\
  Department of Computer Science\\
  New York University\\
  \href{mailto:drr342@nyu.edu}{\texttt{drr342@nyu.edu}}\\
}

\begin{document}

\maketitle

% \cite{} - in-line citation author, year in parenthesis.
% \citep{} - all citation info in parenthesis.

%	\begin{figure}[ht]
%		\centering
%		\frame{
%            \includegraphics[width=1.0\linewidth]{tree.png}
%       }
%		\caption{Classification results for the sentence \textit{"There are slow and repetitive parts, but it has just enough spice to keep it                  interesting."} using the Stanford Sentiment Treebank. As can be seen, sentiment scores are available for each phrase.}
%		\label{tree}
%	\end{figure}

\section{Backprop}

\subsection{Warm-up}
The chain rule is at the heart of backpropagation. Assume you are given input $\bm{x}$ and output $\bm{y}$, both in $\mathbb{R}^2$, and the error backpropagated to the output is $\frac{\partial L}{\partial \bm{y}}$. In particular, let
    \begin{equation} \label{vector}
        \bm{y} = \bm{W}\bm{x} + \bm{b},
    \end{equation}
where $\bm{W} \in \mathbb{R}^{2 \times 2}$ and $\bm{x}, \bm{b} \in \mathbb{R}^2$. Give an expression for$\frac{\partial L}{\partial \bm{W}}$ and $\frac{\partial L}{\partial \bm{b}}$ in terms of $\frac{\partial L}{\partial \bm{y}}$ and $\bm{x}$ using the chain rule.

\textbf{\emph{Solution}}\\
If we let 
\begin{align*}
    \bm{y} &= \begin{bmatrix} y_1 & y_2 \\ \end{bmatrix}^\top \\
    \bm{x} &= \begin{bmatrix} x_1 & x_2 \\ \end{bmatrix}^\top \\
    \bm{b} &= \begin{bmatrix} b_1 & b_2 \\ \end{bmatrix}^\top \\
    \bm{W} &= \begin{bmatrix}
                    W_{1,1} & W_{1,2} \\
                    W_{2,1} & W_{2,2} \\
                \end{bmatrix}
\end{align*}
We can rewrite equation \ref{vector} as follows:
\begin{align*}
    y_1 &= W_{1,1}x_1 + W_{1,2}x_2 + b_1 \\
    y_2 &= W_{2,1}x_1 + W_{2,2}x_2 + b_2
\end{align*}
Obtaining the partial derivatives of these equations and using chain rule it is easy to see that
\begin{align*}
    \frac{\partial L}{\partial W_{i,j}} &= \frac{\partial L}{\partial y_{i}} \cdot \frac{\partial y_i}{\partial W_{i,j}} \\
    &= \frac{\partial L}{\partial y_{i}} \cdot x_j \\
    \frac{\partial L}{\partial b_{i}} &= \frac{\partial L}{\partial y_{i}} \cdot \frac{\partial y_i}{\partial b_{i}} \\
    &= \frac{\partial L}{\partial y_{i}}
\end{align*}
Finally, we can write the vector expressions for these derivatives:
\begin{align*}
    \frac{\partial L}{\partial \bm{W}} &= \begin{bmatrix}
                    \frac{\partial L}{\partial W_{1,1}} & \frac{\partial L}{\partial W_{1,2}} \\
                    \frac{\partial L}{\partial W_{2,1}} & \frac{\partial L}{\partial W_{2,2}} \\
                \end{bmatrix} = \begin{bmatrix}
                    \frac{\partial L}{\partial y_{1}} \cdot x_1 & \frac{\partial L}{\partial y_{1}} \cdot x_2 \\
                    \frac{\partial L}{\partial y_{2}} \cdot x_1 & \frac{\partial L}{\partial y_{2}} \cdot x_2 \\
                \end{bmatrix} \\
    &= \frac{\partial L}{\partial \bm{y}} \cdot \bm{x}^\top \\
    \frac{\partial L}{\partial \bm{b}} &= \begin{bmatrix}
                    \frac{\partial L}{\partial b_{1}} \\
                    \frac{\partial L}{\partial b_{2}} \\
                \end{bmatrix} = \begin{bmatrix}
                    \frac{\partial L}{\partial y_{1}} \\
                    \frac{\partial L}{\partial y_{2}} \\
                \end{bmatrix} \\
    &= \frac{\partial L}{\partial \bm{y}}
\end{align*}

\subsection{Softmax}
Multinomial logistic regression is a generalization of logistic regression into multiple classes. The softmax expression which indicates
the probability of the $j$-th class is as follows:
    \begin{equation*}
        y_j = \frac{\exp(\beta x_j)}{\sum_{k=1}^n \exp(\beta x_k)}
    \end{equation*}
What is the expression for $\frac{\partial y_j}{\partial x_i}$? (Hint: Answer differs when $i = j$ and $i \neq j$).

\textbf{\emph{Solution}}
\begin{enumerate}[I.]
    \item $i \neq j$
    \begin{align*}
        \frac{\partial y_j}{\partial x_i} &= \frac{\partial}{\partial x_i}\left[\frac{\exp(\beta x_j)}{\sum_{k=1}^n \exp(\beta x_k)}\right] \\
        &= \exp(\beta x_j)\left[(-1)\left(\sum_{k=1}^n \exp(\beta x_k)\right)^{-2}\right] \frac{\partial}{\partial x_i}\left[\sum_{k=1}^n \exp(\beta x_k)\right] \\
        &= - \frac{\exp(\beta x_j)}{\left[\sum_{k=1}^n \exp(\beta x_k)\right]^{2}} \left[ \beta \exp(\beta x_i) \right] \\
        &= -\beta \cdot \frac{\exp(\beta x_j)}{\sum_{k=1}^n \exp(\beta x_k)} \cdot \frac{\exp(\beta x_i)}{\sum_{k=1}^n \exp(\beta x_k)} \\
        &= -\beta y_j y_i
    \end{align*}
    \item $i = j$
    \begin{align*}
        \frac{\partial y_j}{\partial x_j} &= \frac{\partial}{\partial x_j}\left[\frac{\exp(\beta x_j)}{\sum_{k=1}^n \exp(\beta x_k)}\right] \\
        &= \exp(\beta x_j) \cdot \frac{\partial}{\partial x_j}\left[\frac{1}{\sum_{k=1}^n \exp(\beta x_k)}\right] + \frac{1}{\sum_{k=1}^n \exp(\beta x_k)} \cdot \frac{\partial}{\partial x_j}\left[\exp(\beta x_j)\right] \\
        &= -\beta y_j^2 + \frac{\beta \exp(\beta x_j)}{\sum_{k=1}^n \exp(\beta x_k)} \\
        &= -\beta y_j^2 + \beta y_j \\
        &= \beta y_j (1 - y_j)
    \end{align*}
\end{enumerate}

\end{document}